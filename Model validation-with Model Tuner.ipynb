{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from model_tuner import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into training and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n",
      " 'mean smoothness' 'mean compactness' 'mean concavity'\n",
      " 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n",
      " 'radius error' 'texture error' 'perimeter error' 'area error'\n",
      " 'smoothness error' 'compactness error' 'concavity error'\n",
      " 'concave points error' 'symmetry error' 'fractal dimension error'\n",
      " 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n",
      " 'worst smoothness' 'worst compactness' 'worst concavity'\n",
      " 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n"
     ]
    }
   ],
   "source": [
    "# features\n",
    "X = dataset.data\n",
    "print(dataset.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['malignant' 'benign']\n"
     ]
    }
   ],
   "source": [
    "# features\n",
    "y = dataset.target\n",
    "print(dataset.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([145, 236])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(y_train.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 67, 121])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.bincount(y_valid.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled=scaler.transform(X_train)\n",
    "X_valid_scaled=scaler.transform(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  40\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs')\n",
    "\n",
    "\n",
    "LR = Pipeline([('LR', clf)])\n",
    "\n",
    "penalties = ['l1', 'l2']\n",
    "Cs = np.logspace(-4, 1, 20)\n",
    "solvers = ['liblinear']\n",
    "\n",
    "parameters = [{'LR__penalty':penalty, 'LR__C': c, 'LR__solver':solver} \n",
    "              for penalty in penalties\n",
    "              for c in Cs\n",
    "              for solver in solvers]\n",
    "\n",
    "print('Number of parameters: ' , len(parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 521.59it/s]\n"
     ]
    }
   ],
   "source": [
    "model = ModelTuner(pipeline=LR,parameters=parameters, X_train=X_train_scaled, \n",
    "                    y_train=y_train.astype(int), X_valid=X_valid_scaled,\n",
    "                    y_valid=y_valid.astype(int), eval_metric=roc_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9983964475144936"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  60\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
    "            decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
    "            max_iter=-1, probability=True, random_state=0, shrinking=True,\n",
    "            tol=0.001, verbose=False)\n",
    "\n",
    "\n",
    "SVM = Pipeline([('SVM', clf)])\n",
    "\n",
    "Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "gammas = [0.001, 0.01, 0.1, 1]\n",
    "kernels = ['rbf','poly','linear']\n",
    "\n",
    "          \n",
    "parameters = [{'SVM__gamma':gamma, 'SVM__C': c, 'SVM__kernel':kernel} \n",
    "              for gamma in gammas\n",
    "              for c in Cs\n",
    "              for kernel in kernels]\n",
    "          \n",
    "print('Number of parameters: ' , len(parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:01<00:00, 56.75it/s]\n"
     ]
    }
   ],
   "source": [
    "model = ModelTuner(pipeline=SVM,parameters=parameters, X_train=X_train_scaled, \n",
    "                    y_train=y_train.astype(int), X_valid=X_valid_scaled,\n",
    "                    y_valid=y_valid.astype(int), eval_metric=roc_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9980263969409152"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest and random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  7920\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
    "            oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
    "\n",
    "\n",
    "RF = Pipeline([('RF', clf)])\n",
    "\n",
    "bootstraps = [True, False]\n",
    "max_depths = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None]\n",
    "max_features = ['auto', 'sqrt']\n",
    "min_samples_leafs = [1, 2, 4]\n",
    "min_samples_splits = [2, 5, 10]\n",
    "n_estimators = np.arange(10,500,50)\n",
    "criteria = ['gini','entropy']\n",
    "\n",
    "          \n",
    "parameters = [{'RF__bootstrap': bootstrap,\n",
    "               'RF__max_depth': depth,\n",
    "               'RF__max_features': feat,\n",
    "               'RF__min_samples_leaf': leaf,\n",
    "               'RF__min_samples_split': split,\n",
    "               'RF__n_estimators': estimators,\n",
    "               'RF__criterion': criterion}\n",
    "\n",
    "              for bootstrap in bootstraps\n",
    "              for depth in max_depths\n",
    "              for feat in max_features\n",
    "              for leaf in min_samples_leafs\n",
    "              for split in min_samples_splits\n",
    "              for estimators in n_estimators\n",
    "              for criterion in criteria]\n",
    "          \n",
    "print('Number of parameters: ' , len(parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of random parameters:  40\n"
     ]
    }
   ],
   "source": [
    "#RANDOM SAMPLING\n",
    "random_inds = np.random.RandomState(20).randint(0,len(parameters),size=40)\n",
    "\n",
    "parameters = np.array(parameters)\n",
    "random_params = parameters[random_inds]\n",
    "\n",
    "print('Number of random parameters: ' , len(random_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:12<00:00,  2.86it/s]\n"
     ]
    }
   ],
   "source": [
    "model = ModelTuner(pipeline=RF,parameters=random_params, X_train=X_train_scaled, \n",
    "                    y_train=y_train.astype(int), X_valid=X_valid_scaled,\n",
    "                    y_valid=y_valid.astype(int), eval_metric=roc_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.997224620698162"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. XGBoost and Random hyper parameter sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  3840\n"
     ]
    }
   ],
   "source": [
    "clf = xgboost.XGBClassifier(\n",
    "             learning_rate =0.1,\n",
    "             n_estimators=1000,\n",
    "             max_depth=5,\n",
    "             min_child_weight=1,\n",
    "             gamma=0,\n",
    "             subsample=0.8,\n",
    "             colsample_bytree=0.8,\n",
    "             objective= 'binary:logistic',\n",
    "             nthread=10,\n",
    "             scale_pos_weight = np.bincount(y_train)[0]/np.bincount(y_train)[1],\n",
    "             seed=27)\n",
    "\n",
    "\n",
    "XGB = Pipeline([('XGB', clf)])\n",
    "\n",
    "\n",
    "learning_rates = [0.05, 0.10, 0.15]\n",
    "max_depths = [6, 8, 10, 12, 15]\n",
    "min_child_weights = [ 1, 3, 5, 7 ]\n",
    "gammas = [ 0.1, 0.2 , 0.3, 0.4 ]\n",
    "colsample_bytrees = [ 0.3, 0.4, 0.5 , 0.7 ]\n",
    "n_estimators = np.arange(100,300,50)\n",
    "\n",
    "\n",
    "\n",
    "parameters = [{  'XGB__learning_rate'    : learning_rate,\n",
    "                 'XGB__max_depth'        : depth,\n",
    "                 'XGB__min_child_weight' : min_child_weight,\n",
    "                 'XGB__gamma'            : gamma,\n",
    "                 'XGB__colsample_bytree' : colsample_bytree,\n",
    "                 'XGB__n_estimators' : estimators}\n",
    "\n",
    "              for learning_rate in learning_rates\n",
    "              for depth in max_depths\n",
    "              for min_child_weight in min_child_weights\n",
    "              for gamma in gammas\n",
    "              for colsample_bytree in colsample_bytrees\n",
    "              for estimators in n_estimators]\n",
    "          \n",
    "print('Number of parameters: ' , len(parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of random parameters:  10\n"
     ]
    }
   ],
   "source": [
    "#RANDOM SAMPLING\n",
    "random_inds = np.unique(np.random.RandomState(20)\\\n",
    "                .randint(0,len(parameters),size=10))\n",
    "\n",
    "parameters = np.array(parameters)\n",
    "random_params = parameters[random_inds]\n",
    "\n",
    "print('Number of random parameters: ' , len(random_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 32.38it/s]\n"
     ]
    }
   ],
   "source": [
    "model = ModelTuner(pipeline=XGB,parameters=random_params, \n",
    "                   X_train=X_train_scaled, \n",
    "                    y_train=y_train.astype(int), \n",
    "                    X_valid=X_valid_scaled,\n",
    "                    y_valid=y_valid.astype(int), eval_metric=roc_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9979030467497225"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
